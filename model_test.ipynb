{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "#from time import time\n",
    "#from tensorflow.python.keras.callbacks import TensorBoard \n",
    "#from keras.callbacks import TensorBoard\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import cartpole_mod\n",
    "\n",
    "# For model rendering\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        self.train_info = []        # for Training accuracy & loss\n",
    "        #self.log_dir = './logs/log_{}'.format(time())\n",
    "        #self.tensorboard = TensorBoard(log_dir='./logs/test', update_freq=128)\n",
    "\n",
    "    \"\"\"Huber loss for Q Learning\n",
    "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
    "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond  = K.abs(error) <= clip_delta\n",
    "\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "\n",
    "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss,\n",
    "                      optimizer=Adam(lr=self.learning_rate),\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        hist_batch = [] # for training accuracy & loss\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                # a = self.model.predict(next_state)[0]\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "                # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "            hist = self.model.fit(state, target, epochs=1, verbose=0)\n",
    "            # hist has the length of batch size\n",
    "            # Mean accuracy & loss should be calculated\n",
    "            hist_batch.append([hist.history['acc'], hist.history['loss']])\n",
    "        hist_batch = np.array(hist_batch)\n",
    "        acc_loss = [np.mean(hist_batch[:,0]), np.mean(hist_batch[:,1])]\n",
    "        self.train_info.append(acc_loss)    # Save the training acc & loss\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def get_history(self):       \n",
    "        data = self.train_info\n",
    "        self.train_info = []  # For new episode, it should be cleared\n",
    "        return data\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02760577  0.00086813 -0.02136235 -0.03615876]\n",
      "[-0.00238031 -0.02575478 -0.00301913  0.00314516]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model_path = \"/preurp/deep-q-learning/save/cartpole-ddqn.h5\"\n",
    "    EPISODES = 1\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.visible_device_list = \"0\"\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "    set_session(tf.Session(config=config))\n",
    "\n",
    "    \n",
    "    \n",
    "    env = gym.make('CartPole-v3')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    agent.load(model_path)\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    \n",
    "    history = {}\n",
    "    history['score'] = []\n",
    "    history['epsilon'] = []\n",
    "    temp_score = 0\n",
    "    temp_epsilon = 0\n",
    "    \n",
    "    # For rendering\n",
    "    frames = []\n",
    "\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        for time in range(500):\n",
    "            # env.render()\n",
    "            frames.append(env.render(mode='rgb_array'))\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            state = next_state\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e, EPISODES, time, agent.epsilon))\n",
    "                temp_score = time\n",
    "                temp_epsilon = agent.epsilon\n",
    "                break\n",
    "\n",
    "        history[e] = agent.get_history()\n",
    "        history['score'].append(temp_score)\n",
    "        history['epsilon'].append(temp_epsilon)\n",
    "        \n",
    "    env.render()\n",
    "    display_frames_as_gif(frames)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
