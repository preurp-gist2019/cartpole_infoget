{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0729 18:24:26.342990 140610635187968 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0729 18:24:26.345597 140610635187968 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0729 18:24:26.349293 140610635187968 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0729 18:24:26.445740 140610635187968 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0729 18:24:26.454203 140610635187968 deprecation.py:323] From <ipython-input-1-3615254fd536>:47: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0729 18:24:26.515376 140610635187968 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0729 18:24:26.516266 140610635187968 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: 12, e: 1.0\n",
      "Best model saved - episode: 0.0, score: 12.0\n",
      "episode: 1/1000, score: 17, e: 1.0\n",
      "Best model saved - episode: 1.0, score: 17.0\n",
      "episode: 2/1000, score: 13, e: 0.89\n",
      "episode: 3/1000, score: 16, e: 0.75\n",
      "episode: 4/1000, score: 23, e: 0.6\n",
      "Best model saved - episode: 4.0, score: 23.0\n",
      "episode: 5/1000, score: 11, e: 0.54\n",
      "episode: 6/1000, score: 9, e: 0.49\n",
      "episode: 7/1000, score: 19, e: 0.4\n",
      "episode: 8/1000, score: 25, e: 0.31\n",
      "Best model saved - episode: 8.0, score: 25.0\n",
      "episode: 9/1000, score: 25, e: 0.24\n",
      "Best model saved - episode: 9.0, score: 25.0\n",
      "episode: 10/1000, score: 83, e: 0.11\n",
      "Best model saved - episode: 10.0, score: 83.0\n",
      "episode: 11/1000, score: 15, e: 0.091\n",
      "episode: 12/1000, score: 109, e: 0.031\n",
      "Best model saved - episode: 12.0, score: 109.0\n",
      "episode: 13/1000, score: 20, e: 0.025\n",
      "episode: 14/1000, score: 74, e: 0.012\n",
      "episode: 15/1000, score: 175, e: 0.0099\n",
      "Best model saved - episode: 15.0, score: 175.0\n",
      "episode: 16/1000, score: 91, e: 0.0099\n",
      "episode: 17/1000, score: 63, e: 0.0099\n",
      "episode: 18/1000, score: 170, e: 0.0099\n",
      "episode: 19/1000, score: 76, e: 0.0099\n",
      "episode: 20/1000, score: 112, e: 0.0099\n",
      "episode: 21/1000, score: 129, e: 0.0099\n",
      "episode: 22/1000, score: 115, e: 0.0099\n",
      "episode: 23/1000, score: 80, e: 0.0099\n",
      "episode: 24/1000, score: 107, e: 0.0099\n",
      "episode: 25/1000, score: 96, e: 0.0099\n",
      "episode: 26/1000, score: 154, e: 0.0099\n",
      "episode: 27/1000, score: 134, e: 0.0099\n",
      "episode: 28/1000, score: 143, e: 0.0099\n",
      "episode: 29/1000, score: 95, e: 0.0099\n",
      "episode: 30/1000, score: 70, e: 0.0099\n",
      "episode: 31/1000, score: 88, e: 0.0099\n",
      "episode: 32/1000, score: 80, e: 0.0099\n",
      "episode: 33/1000, score: 71, e: 0.0099\n",
      "episode: 34/1000, score: 66, e: 0.0099\n",
      "episode: 35/1000, score: 82, e: 0.0099\n",
      "episode: 36/1000, score: 83, e: 0.0099\n",
      "episode: 37/1000, score: 72, e: 0.0099\n",
      "episode: 38/1000, score: 65, e: 0.0099\n",
      "episode: 39/1000, score: 85, e: 0.0099\n",
      "episode: 40/1000, score: 86, e: 0.0099\n",
      "episode: 41/1000, score: 90, e: 0.0099\n",
      "episode: 42/1000, score: 83, e: 0.0099\n",
      "episode: 43/1000, score: 14, e: 0.0099\n",
      "episode: 44/1000, score: 80, e: 0.0099\n",
      "episode: 45/1000, score: 81, e: 0.0099\n",
      "episode: 46/1000, score: 108, e: 0.0099\n",
      "episode: 47/1000, score: 263, e: 0.0099\n",
      "Best model saved - episode: 47.0, score: 263.0\n",
      "episode: 48/1000, score: 220, e: 0.0099\n",
      "episode: 49/1000, score: 126, e: 0.0099\n",
      "episode: 50/1000, score: 101, e: 0.0099\n",
      "episode: 51/1000, score: 144, e: 0.0099\n",
      "episode: 52/1000, score: 180, e: 0.0099\n",
      "episode: 53/1000, score: 107, e: 0.0099\n",
      "episode: 54/1000, score: 99, e: 0.0099\n",
      "episode: 55/1000, score: 150, e: 0.0099\n",
      "episode: 56/1000, score: 259, e: 0.0099\n",
      "episode: 57/1000, score: 290, e: 0.0099\n",
      "Best model saved - episode: 57.0, score: 290.0\n",
      "episode: 58/1000, score: 101, e: 0.0099\n",
      "episode: 59/1000, score: 124, e: 0.0099\n",
      "episode: 60/1000, score: 82, e: 0.0099\n",
      "episode: 61/1000, score: 111, e: 0.0099\n",
      "episode: 62/1000, score: 104, e: 0.0099\n",
      "episode: 63/1000, score: 99, e: 0.0099\n",
      "episode: 64/1000, score: 92, e: 0.0099\n",
      "episode: 65/1000, score: 80, e: 0.0099\n",
      "episode: 66/1000, score: 62, e: 0.0099\n",
      "episode: 67/1000, score: 100, e: 0.0099\n",
      "episode: 68/1000, score: 28, e: 0.0099\n",
      "episode: 69/1000, score: 90, e: 0.0099\n",
      "episode: 70/1000, score: 62, e: 0.0099\n",
      "episode: 71/1000, score: 45, e: 0.0099\n",
      "episode: 72/1000, score: 113, e: 0.0099\n",
      "episode: 73/1000, score: 35, e: 0.0099\n",
      "episode: 74/1000, score: 20, e: 0.0099\n",
      "episode: 75/1000, score: 18, e: 0.0099\n",
      "episode: 76/1000, score: 19, e: 0.0099\n",
      "episode: 77/1000, score: 96, e: 0.0099\n",
      "episode: 78/1000, score: 30, e: 0.0099\n",
      "episode: 79/1000, score: 47, e: 0.0099\n",
      "episode: 80/1000, score: 96, e: 0.0099\n",
      "episode: 81/1000, score: 97, e: 0.0099\n",
      "episode: 82/1000, score: 31, e: 0.0099\n",
      "episode: 83/1000, score: 55, e: 0.0099\n",
      "episode: 84/1000, score: 22, e: 0.0099\n",
      "episode: 85/1000, score: 31, e: 0.0099\n",
      "episode: 86/1000, score: 106, e: 0.0099\n",
      "episode: 87/1000, score: 56, e: 0.0099\n",
      "episode: 88/1000, score: 103, e: 0.0099\n",
      "episode: 89/1000, score: 109, e: 0.0099\n",
      "episode: 90/1000, score: 19, e: 0.0099\n",
      "episode: 91/1000, score: 15, e: 0.0099\n",
      "episode: 92/1000, score: 114, e: 0.0099\n",
      "episode: 93/1000, score: 69, e: 0.0099\n",
      "episode: 94/1000, score: 88, e: 0.0099\n",
      "episode: 95/1000, score: 22, e: 0.0099\n",
      "episode: 96/1000, score: 127, e: 0.0099\n",
      "episode: 97/1000, score: 235, e: 0.0099\n",
      "episode: 98/1000, score: 243, e: 0.0099\n",
      "episode: 99/1000, score: 182, e: 0.0099\n",
      "episode: 100/1000, score: 97, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  89.17\n",
      "episode: 101/1000, score: 79, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  89.79\n",
      "episode: 102/1000, score: 87, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  90.53\n",
      "episode: 103/1000, score: 156, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  91.93\n",
      "episode: 104/1000, score: 100, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  92.7\n",
      "episode: 105/1000, score: 111, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  93.7\n",
      "episode: 106/1000, score: 158, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  95.19\n",
      "episode: 107/1000, score: 133, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  96.33\n",
      "episode: 108/1000, score: 67, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  96.75\n",
      "episode: 109/1000, score: 175, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  98.25\n",
      "episode: 110/1000, score: 16, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  97.58\n",
      "episode: 111/1000, score: 16, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  97.59\n",
      "episode: 112/1000, score: 109, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  97.59\n",
      "episode: 113/1000, score: 12, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  97.51\n",
      "episode: 114/1000, score: 13, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  96.9\n",
      "episode: 115/1000, score: 119, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  96.34\n",
      "episode: 116/1000, score: 111, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  96.54\n",
      "episode: 117/1000, score: 127, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  97.18\n",
      "episode: 118/1000, score: 121, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  96.69\n",
      "episode: 119/1000, score: 138, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  97.31\n",
      "episode: 120/1000, score: 106, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  97.25\n",
      "episode: 121/1000, score: 153, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  97.49\n",
      "episode: 122/1000, score: 135, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  97.69\n",
      "episode: 123/1000, score: 175, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  98.64\n",
      "episode: 124/1000, score: 118, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  98.75\n",
      "episode: 125/1000, score: 111, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  98.9\n",
      "episode: 126/1000, score: 201, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  99.37\n",
      "episode: 127/1000, score: 118, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  99.21\n",
      "episode: 128/1000, score: 284, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  100.62\n",
      "episode: 129/1000, score: 19, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  99.86\n",
      "episode: 130/1000, score: 126, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  100.42\n",
      "episode: 131/1000, score: 122, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  100.76\n",
      "episode: 132/1000, score: 170, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  101.66\n",
      "episode: 133/1000, score: 99, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  101.94\n",
      "episode: 134/1000, score: 175, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  103.03\n",
      "episode: 135/1000, score: 432, e: 0.0099\n",
      "Best model saved - episode: 135.0, score: 432.0\n",
      "Mean Score of previous 100 episodes:  106.53\n",
      "episode: 136/1000, score: 499, e: 0.0099\n",
      "Best model saved - episode: 136.0, score: 499.0\n",
      "Mean Score of previous 100 episodes:  110.69\n",
      "episode: 137/1000, score: 499, e: 0.0099\n",
      "Best model saved - episode: 137.0, score: 499.0\n",
      "Mean Score of previous 100 episodes:  114.96\n",
      "episode: 138/1000, score: 499, e: 0.0099\n",
      "Best model saved - episode: 138.0, score: 499.0\n",
      "Mean Score of previous 100 episodes:  119.3\n",
      "episode: 139/1000, score: 173, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  120.18\n",
      "episode: 140/1000, score: 152, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  120.84\n",
      "episode: 141/1000, score: 126, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  121.2\n",
      "episode: 142/1000, score: 112, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  121.49\n",
      "episode: 143/1000, score: 120, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  122.55\n",
      "episode: 144/1000, score: 120, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  122.95\n",
      "episode: 145/1000, score: 11, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  122.25\n",
      "episode: 146/1000, score: 13, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  121.3\n",
      "episode: 147/1000, score: 11, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  118.78\n",
      "episode: 148/1000, score: 11, e: 0.0099\n",
      "Mean Score of previous 100 episodes:  116.69\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import cartpole_mod\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import json\n",
    "# Version 1.3\n",
    "\n",
    "EPISODES = 1000\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_node = 24\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "        self.train_info = []        # for Training accuracy & loss\n",
    "\n",
    "    \"\"\"Huber loss for Q Learning\n",
    "\n",
    "    References: https://en.wikipedia.org/wiki/Huber_loss\n",
    "                https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n",
    "    \"\"\"\n",
    "\n",
    "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
    "        error = y_true - y_pred\n",
    "        cond  = K.abs(error) <= clip_delta\n",
    "\n",
    "        squared_loss = 0.5 * K.square(error)\n",
    "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
    "\n",
    "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.num_node, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(self.num_node, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=self._huber_loss,\n",
    "                      optimizer=Adam(lr=self.learning_rate),\n",
    "                     metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        hist_batch = [] # for training accuracy & loss\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                # a = self.model.predict(next_state)[0]\n",
    "                t = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(t)\n",
    "                # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
    "            hist = self.model.fit(state, target, epochs=1, verbose=0)\n",
    "            # hist has the length of batch size\n",
    "            # Mean accuracy & loss should be calculated\n",
    "            hist_batch.append([hist.history['acc'], hist.history['loss']])\n",
    "            # self.model.fit(state, target, epochs=1, verbose=0)                 #######필요한지 확인바람\n",
    "        \n",
    "        hist_batch = np.array(hist_batch)\n",
    "        acc_loss = [np.mean(hist_batch[:,0]), np.mean(hist_batch[:,1])]\n",
    "        self.train_info.append(acc_loss)    # Save the training acc & loss\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def get_history(self):       \n",
    "        data = self.train_info\n",
    "        self.train_info = []  # For new episode, it should be cleared\n",
    "        return data\n",
    "            \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "    set_session(tf.Session(config=config))\n",
    "    env = gym.make('CartPole-v3')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    #agent.load(\"./save/cartpole-ddqn.h5\")\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    \n",
    "    \n",
    "    history = {}\n",
    "    history['score'] = []\n",
    "    history['epsilon'] = []\n",
    "    temp_score = 0\n",
    "    temp_epsilon = 0\n",
    "\n",
    "    \n",
    "    records = np.zeros((1,2))\n",
    "    \n",
    "    trialnumber = \"default\"#.format(agent.epsilon) #이부분을 자기가 바꿀것!\n",
    "    \n",
    "    #Make the bestscore file into 1\n",
    "    #i = 1\n",
    "    #f = open(\"./save/cartpole_ddqn_Bestscore_{0}.txt\".format(trialnumber), 'w')\n",
    "    #f.write(\"{}\" .format(i))\n",
    "    #f.close()\n",
    "    \n",
    "    #If bestscore is bigger than 50, epsilon must be 0.1. Only one time\n",
    "    #f = open(\"./save/cartpole_ddqn_Bestscore.txt\", 'r')\n",
    "    #exscore = float(f.readline()) \n",
    "    #f.close()\n",
    "    #if exscore >= 50 :\n",
    "    #    agent.epsilon = 0.1\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        #agent.load(\"./save/cartpole-ddqn.h5\")\n",
    "        #done = False\n",
    "        batch_size = 32\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        temp_epsilon = 0\n",
    "        tmep_score = 0\n",
    "        for time in range(500):\n",
    "            # env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done else -10\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                agent.update_target_model()\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e, EPISODES, time, agent.epsilon))\n",
    "                temp_epsilon = agent.epsilon\n",
    "                temp_score = time\n",
    "                if e == 0:\n",
    "                    records[0] = np.array([e, time])\n",
    "                else:\n",
    "                    records = np.concatenate((records, [[e, time]]), axis=0)\n",
    "                break\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "                \n",
    "        if done is False :\n",
    "            \n",
    "            temp_score = 499\n",
    "            temp_epsilon = agent.epsilon\n",
    "            agent.update_target_model()\n",
    "            records = np.concatenate((records, [[e, temp_score]]), axis=0)\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e, EPISODES, temp_score, agent.epsilon))\n",
    "        #Save the ex-bestscore to continue next time        \n",
    "        if records[-1,1] >= np.amax(records[:,1]):\n",
    "            #f = open(\"./save/cartpole_ddqn_Bestscore_{0}.txt\".format(trialnumber), 'r')\n",
    "            #line = float(f.readline())\n",
    "            #f.close()\n",
    "            #if records[-1,1] >= line :\n",
    "            #    f = open(\"./save/cartpole_ddqn_Bestscore_{0}.txt\".format(trialnumber), 'w')\n",
    "            #    f.write(\"{}\" .format(records[-1,1]))\n",
    "            #    f.close()\n",
    "            agent.save(\"./save/cartpole-ddqn_{0}.h5\".format(trialnumber))\n",
    "            print(\"Best model saved - episode: {}, score: {}\".format(records[-1,0], records[-1,1]))\n",
    "        history[e] = agent.get_history()\n",
    "        history['score'].append(temp_score)\n",
    "        history['epsilon'].append(temp_epsilon)\n",
    "        \n",
    "        if e >= 100:\n",
    "            latest_hundred = np.mean(history['score'][-100:])\n",
    "            print(\"Mean Score of previous 100 episodes: \", latest_hundred)\n",
    "            \n",
    "            if latest_hundred >= 195:\n",
    "                print(\"Training Done!!! Mean Score: \", latest_hundred)\n",
    "                break\n",
    "            \n",
    "    json = json.dumps(history)\n",
    "    f = open(\"./logs/history_{0}.json\".format(trialnumber), 'w')\n",
    "    f.write(json)\n",
    "    f.close()\n",
    "        # if e % 10 == 0:\n",
    "        #     agent.save(\"./save/cartpole-ddqn.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
